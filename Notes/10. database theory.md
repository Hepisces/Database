# 1. database theory

## 1.1. storage
### 1.1.1. File storage
in DBMS, it organizes the file into a set of pages(A page is a fixed-size block of data.).
4-16KB size.
### 1.1.2. page organization
A DBMS can organize pages in different ways: 
- Heap file organization 
- Tree file organization 
- Sequential file organization 
- Hash file organization

We don't need to know what's inside the page at this point.

#### 1.1.2.1. heap file organization
A heap file is an unordered collection of pages/records with tuples that are stored in random order.

Assuming variable-length records, there may be free space in each Block; when performing an INSERT, how do you know which Block has enough free space?

Most DMBSs maintain a free-space map to keep track of the free space of each Block. A common implementation is to use an array where each element represents the free-space ratio of the corresponding block. For example, in PostgreSQL, each element is a Byte, divided by 256 to get the free-space ratio.


### 1.1.3. Contents of Page
Page consists of header and data, where header is meta-data describing the content of the page: 
- page size 
- checksum 
- DBMS version 
- compression information. 
-  ...

### 1.1.4. Page layout 
#### 1.1.4.1. Fixed-Length Records
Assume that we are only storing tuples in the page. 
If the tuples are fixed-length, the only difficulty is to keep track of the deleted tuples, and one solution is to use a free list.
![](attachments/Pasted%20image%2020250615040829.png)

It is undesirable to move records to occupy the space freed by a deleted record, since doing so requires additional block accesses. Since insertions tend to be more frequent than deletions, it is acceptable to leave open the space occupied by the deleted record and to wait for a subsequent insertion before reusing the space. A simple marker on a deleted record is not sufficient, since it is hard to find this available space when an insertion is being done. Thus, we need to introduce an additional structure:Header 

At the beginning of the file, we allocate a certain number of bytes as a file header. The header will contain a variety of information about the file. For now, all we need to store there is the address of the first record whose contents are deleted. We use this first record to store the address of the second available record, and so on. Intuitively, we can think of these stored addresses as `pointers`, since they point to the location of a record. The deleted records thus form a linked list, which is often referred to as a free list.

#### 1.1.4.2. Variable-Length Records
To represent variable length data, (offset, length) is generally used.
![attachments/Pasted image 20250615041319.png](attachments/Pasted%20image%2020250615041319.png)

The most common way to store these variable-length tuples is in slotted pages, where a slot array is used to record the position and size of each tuple.
![](attachments/Screenshot%202025-06-15%20at%2004.13.52.png)

https://www.postgresql.org/docs/current/storage-page-layout.html

### 1.1.5. storage model
The relational model does not require that the DBMS store all the attributes of a tuple in a single page.
- row-storage, also known as n-ary storage model (NSM): The DBMS stores **all the attributes of a tuple contiguously within a page**. 
- column-storage, also known as decomposition storage model (DSM): The DBMS stores **all the single attribute of all tuples contiguously within a page**.


### 1.1.6. large objects storage
Databases often store data much larger than disk blocks, such as the blob and clob types supported by SQL, which store binary and character large objects, respectively. Many databases internally limit the size of records to no more than the block size, but allow records to logically contain large objects by actually storing the large object separately from the other (short) attributes of the record, and by storing a (logical) pointer to the object in the record containing the large object. The large object may be stored as a file in a file system area managed by the database, or as a file structure managed within the database. In the latter case, large objects within the database can optionally be represented using a B⁺-tree file organization to enable efficient access to any location within the object, supporting the reading of the entire object, the specifying of byte ranges, and the insertion and deletion of portions of the object.

### 1.1.7. Metadata
A database has multiple tables, table data is stored in Pages, and multiple Pages are in one file, so when answering SELECT * FROM student, how does the DBMS know which file to query?

Each database maintains a data dictionary that stores the database's metadata.
![](attachments/Pasted%20image%2020250615042010.png)
https://www.postgresql.org/docs/current/limits.html

## 1.2. Index

A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure.

### 1.2.1. category

- ordered index : value based
- hash index

### 1.2.2. ordered index

#### 1.2.2.1. clustering v.s. non-clustering index

If the search code of the index also `determines the order of the records` in the containing file
On the other side, if the index differ from the records, then it will be non-clustering index/secondary index.

The file organization  that clustering index rely on is `sequential file organization`

A table cannot have multiple clustering index, since you can't guarantee that two search codes will exactly match the sorting in the same file, as how clustering is defined

The primary key of the MySQL InnoDB engine is automatically an clustering index; SQL Server supports both; other DBMSs mainly support non-clustering indexes.

#### 1.2.2.2. dense v.s. sparse index

In a dense index, an index entry appears for every search-key value in the file

sparse index cannot be an non-clustering index, because it would throw subsequent indexing out of whack

### 1.2.3. B+ Tree Index

properties:
B+ trees are balanced with heights of order log N; a B+ node is typically within a page (typically 4 KB).

- Each non-leaf node (except the root node) has between n/2 and n children.
- The number of children of the root node is between 2 and n.

practice:
Assuming a pointer of 8 bytes and a search code of 32 bytes, then n is about 100. Assuming 1 million records, then at most one B+ tree node needs to be accessed: 5

### 1.2.4. Hash Index

A hash function is any function that can be used to map data of arbitrary size to fixed-size values

Hash indexes support only = operators, not range queries, as their random nature

Think：Assuming that 1 page (4KB) can hold 100 index entries, then for 100,000,000 tuples, 1,000,000 pages (i.e., 4GB) are needed to hold the index. Given the memory constraints, what should be done?
Build the index based on the index, Multi-level index

## 1.3. Query plan

![attachments/Pasted image 20250605205705.png](attachments/Pasted%20image%2020250605205705.png)
basic steps:

- parsing: Converting SQL statements to internal representations
- optimization: Generate a query plan
- execution: Execute the query plan

### 1.3.1. query optimization

predicate push down i.e. filter first

e.g.

```sql
 select salary
  from instructor 
  where salary < 75000;
```

`where first,` then select , faster than select first

## 1.4. transaction

The term transaction refers to a collection of operations that form a single logical unit of work.

Properties:(ACID)

- Atomicity
- Consistency
- Isolation
- Durability

### 1.4.1. Atomicity

All operations of a transaction are either executed successfully or the partial effect of each unfinished transaction is undone in the event of a failure.

### 1.4.2. Consistency

Execution of a transaction in isolation (i.e., with no other transaction executing concurrently) preserves the consistency of the database.

（Who owns responsibility: Programmer & some foreign key)

[如何理解数据库事务中的一致性的概念？ - sleep deep的回答 - 知乎](https://www.zhihu.com/question/31346392/answer/569142076)
p.s. Here the meaning of the consistency differs from the consistency in distributed database

### 1.4.3. Isolation

In a database system that allows multiple transactions to execute concurrently, if updates to shared data are not controlled, transactions may see inconsistent intermediate state resulting from updates by other transactions. This situation can lead to incorrect updates to data in the database. Therefore, database systems must provide mechanisms to isolate the effects of one transaction from other concurrently executing transactions.

### 1.4.4. Durability

Once a transaction has been successfully executed, its effects must be persisted in the database-a system failure should not cause the database to forget a successfully completed transaction. This feature is called durability.

### 1.4.5. Transaction isolation level

(trade off consistency and parallelization)

- Serializable: Typically ensures serialized execution of transactions
- Repeatable Read: Allow only committed data to be read, and further require that no other transaction be allowed to update a data item between two reads of that item by a transaction
- Read Commited: Only committed data is allowed to be read, but repeated reads are not guaranteed. For example, between two reads of a data item by the same transaction, another transaction may have updated the data item and committed it. (`Default Isolation Levels for Most DBMSs`)
- Read Uncommited: Allow reading of uncommitted data. It is the lowest level of isolation allowed in SQL.

#### 1.4.5.1. some bad condition

- dirty read: A dirty read is when one transaction (e.g., Transaction A) reads data that another transaction (e.g., Transaction B) has modified but not yet committed.(when B rollback it will screw up)
- Non-Repeatable Read: **An unrepeatable read is when the same row** of data is read multiple times within a transaction, but you get different results.(In between multiple reads by transaction A, another transaction B modifies or deletes the row and commits the changes.)
- Phantom Read: A phantom read is when the same range query (e.g., a query with a WHERE clause) is executed multiple times within a single transaction, but the result set returned by the second query **contains rows that didn't exist** at the time of the first query (or some rows are missing, although the emphasis is usually on the **extra rows**).

### 1.4.6. query logic of the transaction
> This comes from a question I asked in a data engineering interview, but there's no guarantee that the answer here is correct


In SQL, transactions are typically managed using the following keywords:

- BEGIN TRANSACTION: starts a transaction.
- COMMIT: commits the transaction, saving the results of all operations to the database.
- ROLLBACK: Rolls back a transaction, undoing all operations in the transaction.

```sql
BEGIN TRANSACTION;

-- Perform some SQL operations
UPDATE accounts SET balance = balance - 100 WHERE account_id = 1;
UPDATE accounts SET balance = balance + 100 WHERE account_id = 2;

-- Check for errors
IF @@ERROR ! = 0
BEGIN
    -- If there is an error, roll back the transaction
    ROLLBACK.
END
ELSE
BEGIN
    -- If there are no errors, commit the transaction
    COMMIT.
END

```

> BTW, I can share another question, which I am still confused after asking all kinds of AI: Essential Differences between MySQL and PostgreSQL?

